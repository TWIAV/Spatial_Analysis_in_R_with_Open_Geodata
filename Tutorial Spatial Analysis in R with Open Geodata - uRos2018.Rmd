---
header-includes:
- \providecommand{\titletxt}{Tutorial - Spatial Analysis in R with Open Geodata}
- \usepackage{listings}
- \usepackage[skins]{tcolorbox}
- \usepackage{float}
- \usepackage{multicol}
- \usepackage{graphicx}
- \usepackage{lastpage}
- \usepackage{fancyhdr}
- \pagestyle{fancy}
- \fancyhf{}
- \lhead{Use of R in Official Statistics - uRos2018}
- \rhead{6th international conference, The Hague, September 2018}
- \usepackage[numbered]{bookmark}
- \usepackage{hyperref}
- \hypersetup{ pdftitle={Tutorial - Spatial Analysis in R with Open Geodata - uRos2018},
  pdfauthor={Egge-Jan Poll√© and Willy Tadema} }
- \hypersetup{linkcolor = {blue}, urlcolor = {blue}}
- \renewcommand{\footrulewidth}{0.4pt}
- \fancyfoot{}
- \fancyfoot[l]{\titletxt}
- \fancyfoot[r]{Page \thepage\ of \pageref*{LastPage}}
- \usepackage{fvextra}
- \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
- \usepackage{xcolor}
- \definecolor{grey}{rgb}{0.8,0.8,0.8}
- \usepackage{enumitem}
- \renewcommand{\figurename}{Figure}
mainfont: Calibri
output:
  pdf_document:
    latex_engine: lualatex
  html_document:
    df_print: paged
urlcolor: blue
---


\newpage

```{r setup, include=FALSE, cache = FALSE}

# Set an output hook to split st_read output so it doesn't extend beyond line width
knitr::knit_hooks$set(
  # modify the output
  output = function(x, options){
    
    # Split in lines
    lines <- unlist(strsplit(x, split = "\n"))
    
    #--------------------------------------------------------------------------*
    # modify the lines we expect to bleed out of the margins ----
    #--------------------------------------------------------------------------*
    
    # For the dsn specification
    lines <- gsub(
      pattern = "' ",
      replacement = "'\n## \t",
      x = lines
    )
    lines <- unlist(sapply(lines, strsplit, split = "\n"))
    
    
    # For the dsn file path
    lines <- ifelse(
      test = grepl("from data source ", lines),
      # Adjust so segments are split in /
      yes = gsub(
        pattern = "/([^/]*)\n",
        replacement = "/\n## \t\\1",
        # Split in fixed width segments
        x = gsub(
          pattern = "(.{,65})",
          replacement = "\\1\n",
          x = lines
        )
      ),
      no = lines
    )
    
    
    # For the proj4 string specifications
    lines <- ifelse(
      test = grepl("proj4string:", lines),
      yes = gsub(
        pattern = "[+]",
        replacement = "\n## \t+",
        x = lines
      ),
      no = lines
    )
    
    # bind the lines
    x <- paste(
      "```",
      paste0(lines, collapse = "\n"),
      "```",
      sep = "\n"
    )
    
    return(x)
  }
)


knitr::opts_chunk$set(tidy.opts = list(width.cutoff = 60), tidy = TRUE, strip.white = FALSE, echo = TRUE)
knitr::opts_chunk$set(error = TRUE)
```

\huge Tutorial - Spatial Analysis in R with Open Geodata

\normalsize

\begin{multicols}{2}

\href{https://www.linkedin.com/in/ejhpolle/}{Egge-Jan PollÈ} - Tensing GIS Consultancy B.V.\\
\href{https://www.linkedin.com/in/willytadema/}{Willy Tadema} - Provincie Groningen  

Version 0.8.0 - November 11, 2018

\subsection{Introduction}


This is the manual for a tutorial session to be delivered at the 6\textsuperscript{th} International \href{https://www.aanmelder.nl/uros2018}{Conference on the Use of R in Official Statistics (uRos2018)} at the Dutch Office for National Statistics (\href{https://www.cbs.nl/en-gb}{CBS}) in the Hague.

\begin{itemize}
\item Date: \textbf{Wednesday, September 12, 2018}  
\item Time: \textbf{9:00 - 12:30}
\item Location: \textbf{Tinbergenzaal} (at CBS, Henri Faasdreef 312, 2492JP The Hague, The Netherlands)
\end{itemize}

\subsection{Traning data}

All datasets used in this training manual do come from publicly accessible sources - so, are really \textit{Open Data}. Loading those data directly into R is part of the exercises. At the time of writing all links used in this book to access the data were valid.

\subsection{Course material available on GitHub}

This training manual is available on our \href{https://github.com/TWIAV/Spatial_Analysis_in_R_with_Open_Geodata}{GitHub repository} - both in \textbf{PDF} and in \textbf{Rmd} file format.  

\includegraphics{Images/github.png}

The whole project is a work in progress, so comments, suggestions, code samples and/or useful links are always welcome: just drop us a line or fork us on GitHub.

\subsection{Prerequisites}

\begin{itemize}
  \item Attendees should bring their own laptop with (64-bit) \href{https://www.r-project.org/}{R}, and preferably also \href{https://www.rstudio.com/}{RStudio}, installed.
  \item No specific prior experience (with R) is required for this introductory session, though some basic experience in one or more of the following fields is certainly helpful: Data Science, Programming/Software Development and/or GIS.
\end{itemize}

\end{multicols}

## Packages to install

These are the packages you will at least need during the course:  
`cbsodataR`, `dplyr`, `httr`, `knitr`, `jsonlite`, `mapview`, `sf`, `stringr`, `tmap` and `units`.

**Pro tip**: use the function `purl()` to extract all the R code from the manual:

```{r eval = FALSE}
library(knitr)
purl("Tutorial Spatial Analysis in R with Open Geodata - uRos2018.Rmd")
```
This will definitely save you a lot of copying and pasting :-)





\begin{figure}[b]
\includegraphics{Images/Frontpage.png}
\end{figure}

\newpage
\pdfbookmark{\contentsname}{toc}
\hypersetup{linkcolor=black}
\setcounter{secnumdepth}{4}
\setcounter{tocdepth}{4}
\tableofcontents
\hypersetup{linkcolor=blue}
\newpage

# Managing Geospatial Vector Data in R {-}  

# The package `sf` (Simple Features for R) \label{simple_features}

To manage spatial data in R in this manual we will be using the library `sf`, a relativle new addition to the R universe. The package was [released on CRAN in January 2017](https://www.r-consortium.org/blog/2017/01/03/simple-features-now-on-cran).

This package provides support for simple features, which is a standardized way to encode spatial vector data.

`sf` links directly to three important geospatial libraries, to unlock their power for use in R:

* GDAL for reading and writing data
* GEOS for geometrical operations
* Proj.4 for projection conversions and datum transformations.

**The package `sf` on CRAN:**  
[https://cran.r-project.org/package=sf](https://cran.r-project.org/package=sf)


If the package is not yet installed, you can install it with the following command:

```{r install_sf, eval = FALSE}
install.packages("sf")
```

To be able to manage your spatial data you will first have to load the package `sf`:

```{r library_sf}
library(sf)
```

\tcbset{coltitle=black, colbacktitle=white, colframe=gray!75!black,colback=white,nobeforeafter}
\begin{tcolorbox}[enlarge by=5mm, hyphenationfix, title=\Large{The real geospatial powers behind \lstinline{sf}}]
\begin{itemize}
\item GDAL: the \textbf{Geospatial Data Abstraction Library} is a translator library for raster and vector geospatial data formats. Website: \url{http://www.gdal.org/}
\item GEOS: the \textbf{Geometry Engine, Open Source} contains the complete functionality of the OpenGIS Simple Features for SQL spatial predicate functions and spatial operators. Website: \url{https://trac.osgeo.org/geos}
\item Proj.4: \textbf{PROJ} is a generic coordinate transformation software, that transforms coordinates from one coordinate reference system (CRS) to another. This includes cartographic projections as well as geodetic transformations. Website: \url{http://proj4.org/}
\end{itemize}
\end{tcolorbox}

\newpage

## `st_as_sf()`: converting a `data.frame` into an `sf` object \label{simplefts}

In this first exercise we will try to get some Dutch airports on the map. Let's start with defining some variables:

```{r}
ap <- "Lelystad Airport"
cd <- "LEY"
class(cd)
lat <- 52.460278
lon <- 5.527222
class(lon)
```

Now we will combine these variables into a `data.frame`:

```{r}
airport <- data.frame(ap, cd, lat, lon, stringsAsFactors = FALSE)
```

Wait, we have found a CSV file with some more airports:

```{r}
NL_Airports <- 
  read.csv("http://www.twiav.nl/files/NL_Airports.csv", stringsAsFactors = FALSE)

NL_Airports
```

Let's add our single airport to this dataset:

```{r}
NL_Airports <- rbind(NL_Airports, airport) 
# Oh yeah, of course: this will generate an error...
```

We can easily solve this issue setting the names equal to eachother:
```{r}
names(airport) <- names(NL_Airports)
```

Now we can `rbind` the two data.frames:

```{r}
NL_Airports <- rbind(NL_Airports, airport)
NL_Airports
class(NL_Airports)
```

Now, how do we convert this `data.frame` into an `sf` object? We have got information on the latitude and the longitude of our airports, and we can use this to create geometry, in this case: points.

Please note: the Coordinate Reference System (CRS) of these lat\\lon coordinates is WGS84, which has been given the EPSG code 4326. In chapter \ref{ChapCRS} we will have a closer look at spatial reference systems.

Now we can use the function `st_as_sf()` to convert our `data.frame` - with `longitude` first:

```{r}
library(sf)

NL_Airports <- st_as_sf(NL_Airports, coords = c("longitude","latitude"), crs = 4326)

class(NL_Airports)
```

Our dataset is now both a `data.frame` and an `sf` object, with all the conveniences which come with this dual status!

\begin{figure}[H]
\centering
\caption{Our first \textbf{sf} object in the \textbf{RStudio Environment pane}}
\vspace{5pt}
\includegraphics[width=172mm]{Images/sf_object_in_environment.png}
\label{sf_object_in_environment}
\end{figure}

\newpage

And when we plot this relatively simple dataset to the screen we can see the sheer beauty of our **Simple Features**: the original columns containing the lat\\lon information have been used to create a column containing points. And this geometry column is stored next to the attribute data, in the very same `data.frame`. Wonderful, isn't it? 

```{r}
NL_Airports
```

Here we are talking only points, but in later exercises we will also see geometry columns containing lines and polygons.

Now we can plot our airports like this:

```{r}
plot(st_geometry(NL_Airports), main = "Airports in the Netherlands", pch = 17)
```

\newpage

## `st_write()`

To share the spatial data in a simple features object with non-R users we have to write it to a spatial database or file format with the function `st_write()`

### Export data to an ESRI Shapefile \label{esrishape}

A well-known format to write to is the ESRI Shapefile format (see Figure \ref{output_shapefile1}):

```{r}
st_write(NL_Airports, "NL_Airports.shp")
```

```{r echo = FALSE}
# Delete the output files to keep the repository clean
unlink("NL_Airports.*")
```

\begin{figure}[H]
\centering
\caption{The shapefile with the corresponding files are written to your Working Directory}
\vspace{5pt}
\includegraphics{Images/output_shapefile.png}
\label{output_shapefile1}
\end{figure}

\tcbset{coltitle=black, colbacktitle=white, colframe=gray!75!black,colback=white,nobeforeafter}
\begin{tcolorbox}[enlarge by=5mm, hyphenationfix, title=\Large{The ESRI Shapefile}]
The shapefile format is a well-known and still rather popular geospatial vector data format for Geographic Information System (GIS) software. It spatially describes vector features - points, lines, and polygons - with attribute data attached. The shapefile is a 'classic' GIS file format in the sense that it stores geometry and attribute data in separate files (in this chapter we will discover that a single shapefile in reality consists of multiple files) as opposed to more modern spatial database or file formats, where geometry and attributes are stored together in a single table or file.\\
\\
The shapefile format has been developed by \href{https://www.esri.com/}{Esri} and over the years has become a \textit{de facto} standard for data interoperability among Esri and other GIS software products.
\end{tcolorbox}

\newpage  

### Export data to a GeoJSON file \label{parGeojsonfile}

Another format we may use is GeoJSON (see Figure \ref{nl_airports_geojson_in_npp1}):

```{r}
st_write(NL_Airports, "NL_Airports.geojson")
```

\begin{figure}[H]
\centering
\caption{The GeoJSON file can be viewed in a text editor}
\vspace{5pt}
\includegraphics{Images/nl_airports_geojson_in_npp.png}
\label{nl_airports_geojson_in_npp1}
\end{figure}

## What about `sp`? (Our suggestion: Forget it!)

When you start googling about spatial analysis with R, you will surely tumble into information about the package `sp`. So why would we not use that one? The answer is simple: `sp` is the predecessor of `sf`.

The package `sp` provides classes and methods for spatial data and yes, for many years, it were these classes and methods you had to use to get your spatial data into R. For a long time, `sp` was at the heart of everything spatial in R. But those days are over now, as `sf` is rapidly taking over.

No, `sp` is not entirely obsolete yet. It is still actively maintained and you can find it on CRAN:

**The package `sp` on CRAN:**  
[https://cran.r-project.org/package=sp](https://cran.r-project.org/package=sp)

So, how can we be so sure about our suggestion to try not to use `sp` then?

That has to do with the way the geometry is handled in both packages. In a way you could compare the way the data is stored using the `sp` package with the Esri shapefile mentioned in paragraph \ref{esrishape}: in an `sp Spatial*` object the attribute data and the geometry are stored separately, whereas in an `sf` object the geometry is stored in a column in the dataframe itself (see paragraph \ref{simplefts}).

Please note: the `sf` package is created by Eder Pebesma, together with Roger Bivand. And Pebesma and Bivand are also the main driving forces behind `sp`. So, also in that sense `sf` is really a succesor - and not a competitor - to `sp`. Apparently we had to go through this `sp` phase first, just like we needed the Esri Shapefile once...

It is Pebesma himself who stated in his first [`sf` vignette](https://cran.r-project.org/web/packages/sf/vignettes/sf1.html): "The package `sf` aims at succeeding `sp` in the long term." And developments in that direction are going lightning-fast.

So, let's look forward

### No further reading

Throughout this manual you will find suggestions for further reading, but for the title below we really suggest **no** further reading:

* **Bivand, Roger S., Edzer Pebesma and Virgilio GÛmez-Rubio** (2013), *Applied Spatial Data Analysis with R*. Second Edition. Springer-Verlag New York.

Why? This book was the standard source on spatial data analysis with R, written by the creators of the `sp` package, written in a time that thinking about `simple features` had barely begun. If you manage to lay your hand on a copy, we would surely recommend you to browse through it, if only for historical reasons. But as explained above, your main focus for now should be the future-proof package `sf`.

## Further reading

The big announcement:

* **Pebesma, Edzer** (January 3, 2017). *Simple Features Now on CRAN*. Blog. Retreived from: [https://www.r-consortium.org/blog/2017/01/03/simple-features-now-on-cran](https://www.r-consortium.org/blog/2017/01/03/simple-features-now-on-cran)

The `sf` vignettes:

* [Simple Features for R](https://cran.r-project.org/web/packages/sf/vignettes/sf1.html)
* [Reading, Writing and Converting Simple Features](https://cran.r-project.org/web/packages/sf/vignettes/sf2.html)
* [Manipulating Simple Feature Geometries](https://cran.r-project.org/web/packages/sf/vignettes/sf3.html)
* [Manipulating Simple Features](https://cran.r-project.org/web/packages/sf/vignettes/sf4.html)
* [Plotting Simple Features](https://cran.r-project.org/web/packages/sf/vignettes/sf5.html)
* [Miscellaneous](https://cran.r-project.org/web/packages/sf/vignettes/sf6.html)

\newpage

# Interactive Viewing of Spatial Data in R

In the previous paragraph we have seen a static map plotted on the **Plots** pane. But as a real Data Scientist we also want to be able to explore our data on an interactive map. Until a few years ago this would have meant switching back and forth between a desktop GIS and R. But in recent years some new packages have been developed to enable interactive map viewing in R.

In this paragraph we will look at two of these.

## The package `mapview`

**The package `mapview` on CRAN:**  
[https://cran.r-project.org/package=mapview](https://cran.r-project.org/package=mapview)

If the package is not yet installed, you can install it with the following command:

```{r, eval = FALSE}
install.packages("mapview")
```

To be able to view your spatial data interactively you will first have to load the package `mapview`:

```{r}
library(mapview)
```

Now we can open up an interactive map using a the function `mapview()` (see Figure \ref{Interactive_mapping_mapview1}):

```{r eval = FALSE}
mapview(NL_Airports, color = "red", col.regions = "orange", alpha.regions = 1, label = NL_Airports$airport)
```

\begin{figure}[H]
\centering
\caption{An interactive map can be opened up using a simple mapview() statement}
\vspace{5pt}
\includegraphics{Images/Interactive_mapping_mapview.png}
\label{Interactive_mapping_mapview1}
\end{figure}


\newpage

## The package `tmap`

**The package `tmap` on CRAN:**  
[https://cran.r-project.org/package=tmap](https://cran.r-project.org/package=tmap)

If the package is not yet installed, you can install it with the following command:

```{r, eval = FALSE}
install.packages("tmap")
```

To be able to view your spatial data interactively you will first have to load the package `tmap`:

```{r}
library(tmap)
```

### More on `tmap` I: Workshop Plotting spatial data in R

This afternoon you will be able to see much more thematic mapping as Martijn Tennekes, the author of the `tmap` package, will deliver his workshop **Plotting spatial data in R**.

* Date: Wednesday, September 12, 2018
* Time: 13:30 - 17:00
* Location: this very same Tinbergenzaal

Workshop materials can be found here: [https://github.com/mtennekes/tmap-workshop](https://github.com/mtennekes/tmap-workshop)

\newpage

### More on `tmap` II: Visualizing population density in France (2016)

The thematic map below (see Figure \ref{example_tmap}) shows the population density in France in 2016, with an additional inset for *Œle-de-France*, the region around Paris, as almost 20 percent of the French population lives there.

The code to recreate this map can be found in this blog: [`tmap`: quick and easy thematic mapping in R](http://twiav.nl/en/blog0002en.php)

\begin{figure}[H]
\caption{An example of a thematic map created with tmap}
\vspace{5pt}
\includegraphics{Images/tmap_pop_density_france2016.png}
\label{example_tmap}
\end{figure}

\newpage

## Further reading

The `mapview` vignettes:

* [1. mapview basics](https://r-spatial.github.io/mapview/articles/articles/mapview_01-basics.html)
* [2. mapview advanced controls](https://r-spatial.github.io/mapview/articles/articles/mapview_02-advanced.html)
* [3. mapview options](https://r-spatial.github.io/mapview/articles/articles/mapview_03-options.html)
* [4. mapview popups](https://r-spatial.github.io/mapview/articles/articles/mapview_04-popups.html)
* [5. extra functionality](https://r-spatial.github.io/mapview/articles/articles/mapview_05-extras.html)
* [6. extra leaflet functionality](https://r-spatial.github.io/mapview/articles/articles/mapview_06-add.html)
* [7. ceci constitue la fin du pipe](https://r-spatial.github.io/mapview/articles/articles/mapview_07-pipe.html)

The `tmap` vingette:

* [tmap: get started!](https://cran.r-project.org/web/packages/tmap/vignettes/tmap-getstarted.html)

\newpage

# Accessing Geospatial Vector Data over the Internet {-}

# Downloadable shapefiles

\begin{multicols}{2}

In this chapter we will learn how to download, to unzip and to load shapefiles \textbf{using R}. We have seen the ESRI Shapefile before in paragraph \ref{esrishape}. There will be no need to use your web browser, your file explorer or a zip utility - the whole process can be completed using just a few lines of R code.

It is not uncommon for public organizations - including national statistical institutes - to distribute geographic information in this shapefile format. So, you will find shapefiles on the \href{https://www.cbs.nl/nl-nl/dossier/nederland-regionaal/geografische-data}{page with geographic data of the Dutch Office for National Statistics (CBS)} and also on the \href{http://data.statistik.gv.at/web/catalog.jsp}{STATISTIK AUSTRIA open.data Portal}.

\includegraphics{Images/logo_statistik.png}

\end{multicols}

## Download and unzip the shapefile

```{r results = 'hide'}
# Store the URL to the file to download in a variable
URL2zip <- "http://data.statistik.gv.at/data/OGDEXT_GEM_1_STATISTIK_AUSTRIA_20180101.zip"

# Create a temporary file 
zip_file <- tempfile(fileext = ".zip")

# Download the file
download.file(URL2zip, destfile = zip_file, mode = "wb")

# Create a subfolder in your working directory to store the unzipped data
dir.create("./Data", showWarnings = FALSE)

# Unzip the file
unzip(zip_file, exdir = "./Data")
      
# After unzipping you can delete (i.e. unlink) the file
unlink(zip_file)

# Remove variables you do not longer need
rm(URL2zip, zip_file)
```

\newpage

## Load the shapefile

```{r}
library(sf)
library(tmap)
AUSTRIA_GEM_20180101 <- st_read("./Data/STATISTIK_AUSTRIA_GEM_20180101.shp")
```

```{r}
qtm(AUSTRIA_GEM_20180101)
```

```{r echo = FALSE}
# Delete the Data directory to keep the repo clean
unlink("./Data", recursive = TRUE)
```

\newpage

# OGC Web Feature Service (WFS) \label{chapWFS}

## Introduction

\begin{multicols}{2}

In this chapter we will learn how \textbf{to use R as a client to access data using a Web Feature Service (WFS)}. WFS is a Data Access Standard which is defined and maintained by the Open Geospatial Consortium (OGC). The WFS Interface Standard defines a set of interfaces for accessing geographic information over the Internet. It offers the means to retrieve geographic features and their properties through a highly configurable interface and in a manner independent of the underlying data stores they publish.

The standard is used - both in the public and private sector and in the academic world - to publish vector geospatial datasets in a way that makes it easy for receiving organisations to conduct analysis on the data supplied.

In a short chapter like this it will not be possible to cover all ins and outs of the WFS Interface Standard. The main goal here is to get you up and running and to whetten your appetite for more. It might all look a bit technical and intimidating at first, but as soon as you have the syntax of your request right, you will be able to retrieve valuable spatial data in the blink of an eye.

\end{multicols}

## WFS Outputformat: GML vs. GeoJSON \label{gmlGEOJSON}

By default, a WFS returns data in Geography Markup Language (GML) which is written as eXtensible Markup Language (XML). However, many WFS services also offer the option to request the ouput in GeoJSON, a geospatial data interchange format based on JavaScript Object Notation (JSON).

By it's very nature, GML data is difficult to process, because - as with most XML based grammars - there are two parts to the grammar: the schema that describes the document and the instance document that contains the actual data.

On the contrary, the GeoJSON standard clearly defines several types of JSON objects and the manner in which they are combined to represent data about geographic features, their properties, and their spatial extents.

In general **GDAL**, the translator library behind the `sf` functions `st_read()` and `st_write` (see chapter \ref{simple_features}), gives better results with GeoJSON as opposed to GML.

So, in the exercises in this manual, when accessing a WFS service to retrieve data, we will always add the parameter `outputFormat=application/json`.

## Access a WFS service: `request=GetCapabilities` \label{parGetCap_I}

In general, organisations publishing data using WFS, will provide you with a URL to their WFS server which also contains a `GetCapabilities` request. Once you know the **capabilities** of the service (i.e. once you know what is on offer) you can start building your own request to the server.

In the exercises below we will use data on the municipal division in the Netherlands. These data are offered by the host of this conference, the Dutch statistical office, through [Publieke Dienstverlening op de Kaart](https://www.pdok.nl/) (PDOK) and the [Nationaal Georegister](http://www.nationaalgeoregister.nl/) (NGR).

Some background information on the dataset used, can be found here: [Dataset: CBS Gebiedsindelingen](https://www.pdok.nl/introductie?articleid=1951759).

The actual URL with the `GetCapabilities` reguest is:  
[https://geodata.nationaalgeoregister.nl/cbsgebiedsindelingen/wfs?request=GetCapabilities](https://geodata.nationaalgeoregister.nl/cbsgebiedsindelingen/wfs?request=GetCapabilities)

When you click this link the server response will be shown in your browser in XML format.

We are not going to study this XML response line-by-line. Not now. But please leave the web page open in your browser for later reference.

We will have a closer look at the capabilities document in paragraph \ref{parGetCap_II}

## Retrieve data from a WFS service: `request=GetFeature` \label{GetFeaturePar}

### `request=GetFeature` - an example from the Netherlands \label{GFnl}

A WFS server responding to a *GetFeature* request returns a collection of geographic feature instances filtered according to a criteria set by the requesting client. We will start with a simple request to download a full feature collection without any constraints to filter the content by. The GetFeature request queries the server with a set of parameters, which are concatenated to the URL with an *ampersand* (&).

In the script below we use the `httr` package. This allows us to store the different parameters of our request in a list, only to build the full URL at the end. We do so for readability reasons and to allow for easy modification of our request at a later stage if necessary. And the function `build_url()` will return a properly encoded URL.

A WFS service can offer one or more feature collections, see the `<FeatureTypeList>` section in the XML response to the `GetCapabilities` request. The service we are accessing here offers quite some feature collections, i.e. multiple regional divisions for the years 1995 up to the current year. What we are interested in here, is the municipal division for the year 2017. After some browsing through the XML response, we have found a `<FeatureType>` with the `<Name>` **cbsgebiedsindelingen:cbs_gemeente_2017_gegeneraliseerd**. And that's the value we are giving to the `typename` parameter. (This `typenames` parameter determines the collection of feature instances to return.)

Also, do not forget to add a parameter to ask for output in GeoJSON format (as discussed in paragraph \ref{gmlGEOJSON}).

This is the script to populate the full request:

```{r}
# NL: Example with Dutch data
library(httr)
library(sf)
library(tmap)

url <- parse_url("https://geodata.nationaalgeoregister.nl/cbsgebiedsindelingen/wfs")
url$query <- list(service = "WFS",
                  version = "2.0.0",
                  request = "GetFeature",
                  typename = "cbsgebiedsindelingen:cbs_gemeente_2017_gegeneraliseerd",
                  outputFormat = "application/json")
request <- build_url(url)
```

The variable `request` will now contain this [link](https://geodata.nationaalgeoregister.nl/cbsgebiedsindelingen/wfs/?service=WFS&version=2.0.0&request=GetFeature&typename=cbsgebiedsindelingen%3Acbs_gemeente_2017_gegeneraliseerd&outputFormat=application%2Fjson). (When you click this link the server response will be shown in your browser in GeoJSON format.)

Now we want to feed this response directly into R, like this:

```{r}
NL_Municipalities2017 <- st_read(request)
```

\newpage
Have a look at the result:

```{r}
qtm(NL_Municipalities2017)
head(NL_Municipalities2017)
```
\newpage

### `request=GetFeature` - an example from Finland


In this paragraph we will retrieve data from [Tilastokeskus (Statistics Finland)](http://www.stat.fi/). In the English section of their website we found a nice dataset: [Population by municipality-based units](http://www.stat.fi/org/avoindata/paikkatietoaineistot/vaesto_tilastointialueittain_en.html).


\begin{multicols}{2}
And from this map it is especially the layer \textbf{Population 2017 by municipalities 2018 (kunta\_vaki2017)} we would like to investigate.

We will follow the same steps as in the Dutch example above. For an explanation of the steps taken, please refer to paragraph \ref{parGetCap_I} and \ref{GFnl}.

\includegraphics{Images/tklogo_fi.png}

\end{multicols}

The URL with `GetCapabilities` request is:  
[http://geo.stat.fi/geoserver/vaestoalue/wfs?request=GetCapabilities](http://geo.stat.fi/geoserver/vaestoalue/wfs?request=GetCapabilities)

In this XML response - in the `<FeatureTypeList>` section - we have found a `<FeatureType>` with the `<Name>` **vaestoalue:kunta_vaki2017**. That's the value we are giving to the `typename` parameter.

The script to build the full URL is similar to the one before - the only parameters we have modified are the hostname and the typename:

```{r}
# FI: Example with Finnish data
library(httr)
library(sf)
library(tmap)

url <- parse_url("https://geo.stat.fi/geoserver/vaestoalue/wfs")
url$query <- list(service = "WFS",
                  version = "2.0.0",
                  request = "GetFeature",
                  typename = "vaestoalue:kunta_vaki2017",
                  outputFormat = "application/json")
request <- build_url(url)
```

The variable `request` will now contain this [link](https://geo.stat.fi/geoserver/vaestoalue/wfs/?service=WFS&version=2.0.0&request=GetFeature&typename=vaestoalue%3Akunta_vaki2017&outputFormat=application%2Fjson). (When you click this link the server response will be shown in your browser in GeoJSON format.)

Now we want to feed this response directly into R, like this:

```{r}
FI_Municipalities2018_Pop2017 <- st_read(request)
```
\newpage

Have a look at the result:

```{r}
qtm(FI_Municipalities2018_Pop2017)
```

Optionally, you can have a look at the attribute data in the **Data Viewer**: `View(FI_Municipalities2018_Pop2017)`

\begin{multicols}{2}

With some basic knowledge of the Finnish language you should now be able to calculate that Finland had more or less 5.5 million inhabitants in 2017 - of which more or less 50 percent are males, and the other half females:

\tcbset{coltitle=black, colbacktitle=white, colframe=gray!75!black,colback=white,nobeforeafter}
\begin{tcolorbox}[enlarge by=5mm, hyphenationfix, title=Finnish for Data Scientists: some useful words]
\begin{itemize}
\item \textbf{Miehet} = Men
\item \textbf{Naiset} = Women
\item \textbf{Nimi} = Name
\item \textbf{Vaesto} = Population
\end{itemize}
\end{tcolorbox}

\end{multicols}

```{r}
sum(FI_Municipalities2018_Pop2017$vaesto)
sum(FI_Municipalities2018_Pop2017$miehet)
sum(FI_Municipalities2018_Pop2017$naiset)
```

\newpage

## Get a description of a dataset: `request=DescribeFeatureType` \label{parDescFeatType}

In the previous exercises we immediately executed `GetFeature` requests to WFS services - mainly because the trainer told us to do so - without actually knowing anything about these datasets.

To get a description of the dataset, you can execute a `DescribeFeatureType` request first. This returns a description - in XML format - of the structure, including properties, of the feature type specified in the request.

For the Dutch municipalities dataset this request would look like this:
[https://geodata.nationaalgeoregister.nl/cbsgebiedsindelingen/wfs?service=WFS&version=2.0.0&request=DescribeFeatureType&typename=cbsgebiedsindelingen:cbs_gemeente_2017_gegeneraliseerd](https://geodata.nationaalgeoregister.nl/cbsgebiedsindelingen/wfs?service=WFS&version=2.0.0&request=DescribeFeatureType&typename=cbsgebiedsindelingen:cbs_gemeente_2017_gegeneraliseerd)

And for the Finnish dataset like this:
[http://geo.stat.fi/geoserver/vaestoalue/wfs?service=WFS&version=2.0.0&request=DescribeFeatureType&typename=vaestoalue:kunta_vaki2017](http://geo.stat.fi/geoserver/vaestoalue/wfs?service=WFS&version=2.0.0&request=DescribeFeatureType&typename=vaestoalue:kunta_vaki2017)

## Additional parameters to the `GetFeature` request

Additional parameters can be added to a `GetFeature` request to further filter or convert the response from the WFS.

To include additional parameters to a request, simply add an *ampersand* (&) at the end of the URL, then add the name of the parameter, an equal sign (=) and the value to assign to the parameter. Of course we will not do this manually; we will use the `httr` package to build this URL with additional parameters for us.

Below we will discuss a few of the parameters available.

### Limit the number of records returned: the `count` parameter

To have a look at the structure of a dataset - before retrieving the full dataset - you can choose to limit the number of records returned with the `count` parameter.

The `GetFeature` request below limits the number of Finnish municipalities returned to only 5:

```{r}
# FI: Example with Finnish data
library(httr)

url <- parse_url("https://geo.stat.fi/geoserver/vaestoalue/wfs")
url$query <- list(service = "WFS",
                  version = "2.0.0",
                  request = "GetFeature",
                  typename = "vaestoalue:kunta_vaki2017",
                  count = 5,
                  outputFormat = "application/json")
request <- build_url(url)
```
The variable `request` will now contain this [link](https://geo.stat.fi/geoserver/vaestoalue/wfs/?service=WFS&version=2.0.0&request=GetFeature&typename=vaestoalue%3Akunta_vaki2017&count=5&outputFormat=application%2Fjson). (When you click this link the server response will be shown in your browser in GeoJSON format.)

\newpage

### Limit the number of columns returned: the `PropertyName` parameter

To restrict a `GetFeature` request by attribute rather than feature, use the `PropertyName` parameter. You can specify a single attribute, or multiple attributes separated by commas.

The `GetFeature` request below only returns the geometry and the columns *nimi* (Name) and *vaesto* (total population) for the Finnish municipalities:

```{r}
# FI: Example with Finnish data
library(httr)

url <- parse_url("https://geo.stat.fi/geoserver/vaestoalue/wfs")
url$query <- list(service = "WFS",
                  version = "2.0.0",
                  request = "GetFeature",
                  typename = "vaestoalue:kunta_vaki2017",
                  propertyname = "geom,nimi,vaesto",
                  outputFormat = "application/json")
request <- build_url(url)
```
The variable `request` will now contain this [link](https://geo.stat.fi/geoserver/vaestoalue/wfs/?service=WFS&version=2.0.0&request=GetFeature&typename=vaestoalue%3Akunta_vaki2017&propertyname=geom%2Cnimi%2Cvaesto&outputFormat=application%2Fjson). (When you click this link the server response will be shown in your browser in GeoJSON format.)

\newpage

## Advanced topics \label{parGetCap_II}

In paragraph \ref{parGetCap_I} we have had a quick look at the answer to a `GetCapabilities` request. Here we will dive a little deeper into the information we can retrieve from this capabilities document.

### outputFormat

Not every WFS service supports GeoJSON as an output format. So how can we retrieve the output formats that are available? To answer this question we take a closer look at the capabilities document.

```{r message = FALSE}
library(httr)
library(xml2)

url <- parse_url("https://geodata.nationaalgeoregister.nl/cbsgebiedsindelingen/wfs")
url$query <- list(service = "WFS",
                  version = "2.0.0",
                  request = "GetCapabilities")
request <- build_url(url)
doc <- GET(request) %>% content(as = "text", encoding = "UTF-8") %>% read_xml() 
xpath <- paste0("//ows:Operation[@name='GetFeature']",
                "/ows:Parameter[@name='outputFormat']",
                "/ows:AllowedValues/ows:Value")
output_formats <- doc %>% xml_find_all(xpath) %>% xml_text()
output_formats
```

As we can see, GML and GeoJSON are not the only supported formats for this specific WFS. The service can also return the query result in a CSV or KML file.

\newpage

### FeatureType

To get a list of all available feature types in a service, we can also inspect the capabilities document. Think of a feature type as a layer or dataset.

```{r}
xpath  <- "//wfs:FeatureType/wfs:Name"
feature_types  <- doc %>% xml_find_all(xpath) %>% xml_text()
head(feature_types) 
```

### maxRecordCount

Often the number of records returned by a service per request is limited to prevent performance issues caused by heavy requests. Therefore we should always check the maximum record count per request.

```{r}
xpath <- "//ows:Constraint[@name='CountDefault']/ows:DefaultValue"
maxRecordCount <- doc %>% xml_find_first(xpath) %>% xml_integer()
maxRecordCount
```

So a request to the cbsgebiedsindelingen WFS will never return more than `r format(maxRecordCount, big.mark = ",")` records.

To determine the number of hits before we actually fetch the query result, we can add `resultType=hits` to our `GetFeature` request: 

```{r}
url$query <- list(service = "WFS",
                  version = "2.0.0",
                  request = "GetFeature",
                  typename = "cbsgebiedsindelingen:cbs_arbeidsmarktregio_2014_gegeneraliseerd",
                  resultType = "hits") 
request <- build_url(url)
doc <- doc <- GET(request) %>% content(as = "text", encoding="UTF-8") %>% read_xml() 
xpath <- "//wfs:FeatureCollection/@numberMatched"
hits <- doc %>% xml_find_first(xpath) %>% xml_integer()
hits
```

In this case the number of hits (`r format(hits, big.mark = ",")`) is smaller than the maximum record count per request (`r format(maxRecordCount, big.mark = ",")`). So we can fetch the entire dataset with one request.

\newpage

Let's put it all together in another example and query the Dutch Addresses en Buildings Register (Basisregistratie Adressen en Gebouwen) for the addresses in Hoek van Holland, a seaside town not far from The Hague.

```{r}
url <- parse_url("https://geodata.nationaalgeoregister.nl/bag/wfs")
url$query <- list(service = "WFS",
                  version = "2.0.0",
                  request = "GetCapabilities")
request <- build_url(url)
doc <- GET(request) %>% content(as = "text", encoding="UTF-8") %>% read_xml() 
xpath <- "//wfs:FeatureType/wfs:Name"
feature_types <- doc %>% xml_find_all(xpath) %>% xml_text()
feature_types

xpath <- "//ows:Constraint[@name='CountDefault']/ows:DefaultValue"
maxRecordCount <- doc %>% xml_find_first(xpath) %>% xml_integer()
maxRecordCount

url$query <- list(service = "WFS",
                  version = "2.0.0",
                  request = "GetFeature",
                  typename = "bag:verblijfsobject",
                  cql_filter = "bag:woonplaats='Hoek van Holland'",
                  resultType = "hits")
request <- build_url(url)
doc <- GET(request) %>% content(as = "text", encoding="UTF-8") %>% read_xml()
xpath <- "//wfs:FeatureCollection/@numberMatched"
hits <- doc %>% xml_find_first(xpath) %>% xml_integer()
hits
```

Notice that we added `cql_filter=bag:woonplaats='Hoek van Holland'` to the request to filter out only addresses in Hoek van Holland.

Now the number of hits (`r format(hits, big.mark = ",")`) is greater than the maximum record count per request. We have hit the server limit. So how do we get all addresses if a request will not return more than `r format(maxRecordCount, big.mark = ",")`? The answer is simple: create more requests. This is called paging.

\newpage

### Paging

For paging the parameters `sortBy`, `startIndex` and `count` come in handy. By adding `sortBy=bag:identificatie` to our request we ensure that the features returned, are sorted by their globally unique identifier. `startIndex` specifies the index of the first record. `count` is the number of records that the request should try to fetch. It is identical to the maximum record count.    

Because we know the maximum record count and the number of hits, we can calculate the number of requests required to fetch all records in the query result. By increasing the `startIndex` for each subsequent request, we can eventually retrieve all addresses in Hoek van Holland.

```{r, message = FALSE, warning = FALSE, results = 'hide'}
library(sf)

url$query <- list(service = "wfs", 
                  version = "2.0.0", 
                  request = "GetFeature",
                  typename = "bag:verblijfsobject",
                  cql_filter = "woonplaats='Hoek van Holland'",
                  outputFormat = "application/json",
                  resultType = "results",
                  count = maxRecordCount,
                  sortBy = "bag:identificatie")

requestAddresses <- function(x) {
  url$query$startIndex <- x
  request <- build_url(url) 
  st_read(request, stringsAsFactors = FALSE)
}

addresses <- lapply(seq(0, hits, maxRecordCount), requestAddresses) %>% do.call(rbind, .)
```

```{r}
nrow(addresses)
```

All `r nrow(format(addresses, big.mark = ","))` addresses in Hoek van Holland are fetched!

\newpage

To wrap it up, let's visualize these addresses in an interactive map (see Figure \ref{HvH_interactive_map}):
.

```{r eval = FALSE}
library(tmap)

tmap_mode('view')
tm_shape(addresses) +  tm_dots(col = "black", scale = 0.1) + 
  tm_legend(show = FALSE) + tm_view(basemaps = 'OpenStreetMap')
```

\begin{figure}[H]
\centering
\caption{The BAG addresses in Hoek van Holland on an interactive map}
\vspace{5pt}
\includegraphics{Images/Hoek_van_Holland.png}
\label{HvH_interactive_map}
\end{figure}



## Further reading

* The official OGC Web Feature Service (WFS) Implementation Specification can be found here: [http://www.opengeospatial.org/standards/wfs](http://www.opengeospatial.org/standards/wfs)
* The OGC offers an official tutorial module - [OGC E-learning](http://cite.opengeospatial.org/pub/cite/files/edu/index.html) - covering it's activities and the several standards it maintains. The [specific chapter on WFS](http://cite.opengeospatial.org/pub/cite/files/edu/wfs/text/basic-index.html) can be found [here](http://cite.opengeospatial.org/pub/cite/files/edu/wfs/text/basic-index.html).

\newpage

\begin{figure}[H]
\centering
\caption{On the overview page metadata about the \textbf{Feature Layer} \textit{USA States (Generalized)} is presented}
\vspace{5pt}
\includegraphics{Images/Feature_Layer_USA_States_Metadata.png}
\label{USA_States_AGOL_Metadata}
\end{figure}
\begin{figure}[H]
\centering
\caption{Here the \textbf{Feature Layer} \textit{USA States (Generalized)} is shown in the \textbf{ArcGIS Online Map Viewer}}
\vspace{5pt}
\includegraphics{Images/Feature_Layer_USA_States_Map_Viewer.png}
\label{USA_States_AGOL_MapViewer}
\end{figure}

\newpage

# ArcGIS REST Service

## Introduction

\begin{multicols}{2}

In this chapter we will learn how \textbf{to use R as a client to access data using an ArcGIS REST Service}. We will be accessing ArcGIS Server to retrieve Feature Layers using the \href{https://developers.arcgis.com/rest/}{ArcGIS REST API}. This API is part of ArcGIS Online,  the "complete SaaS mapping platform" by the American GIS company \href{https://www.esri.com/en-us/home}{Esri}. And as part of this platform Esri has launched the \href{https://livingatlas.arcgis.com/en/}{ArcGIS Living Atlas of the World}\footnote{Some background information on the Living Atlas of the World can be found in this \href{http://www.esri.com/esri-news/arcuser/summer-2015/welcome-to-the-living-atlas-of-the-world}{article}, where Esri also invites YOU to \textit{"Become a user and a contibutor"}.}

Some datasets in this Living Atlas are Open Data. Some, but by no means all, as many datasets are only accessible via an ArcGIS Online organizational account. But in cases where the source data is already Open Data, Esri publishes the data under the same conditions.

\includegraphics{Images/living-atlas-logo-size-4.png}

\end{multicols}

## Exploring data in the ArcGIS Living Atlas of the World

After some browsing through the content of the Living Atlas we have found a nice (and open) dataset for this exercise: the **Feature Layer** *USA States (Generalized)*. An overview page with metadata for the dataset we have chosen can be found via this [link](https://www.arcgis.com/home/item.html?id=99fd67933e754a1181cc755146be21ca#overview) (see Figure \ref{USA_States_AGOL_Metadata}). 

Before actually importing the data into R, you can explore the dataset - both the geometry and the attribute data - in the [ArcGIS Online Map Viewer](https://www.arcgis.com/home/webmap/viewer.html?useExisting=1&layers=99fd67933e754a1181cc755146be21ca) (see Figure \ref{USA_States_AGOL_MapViewer}).

## Query the Feature Service

On the overview page we have found the Service URL pointing to the layer **USA_States_Generalized** in the the Services Directory (see Figure \ref{USA_States_AG_REST_Services_Dir}): 

[https://services.arcgis.com/P3ePLMYs2RVChkJx/arcgis/rest/services/USA_States_Generalized/FeatureServer/0](https://services.arcgis.com/P3ePLMYs2RVChkJx/arcgis/rest/services/USA_States_Generalized/FeatureServer/0)

One of the **Supported Operations** for this layer is **Query**. The *Query* page (see Figure \ref{USA_States_AG_REST_Services_Dir_Query}) offers an interface to construct the query: 

[https://services.arcgis.com/P3ePLMYs2RVChkJx/arcgis/rest/services/USA_States_Generalized/FeatureServer/0/query](https://services.arcgis.com/P3ePLMYs2RVChkJx/arcgis/rest/services/USA_States_Generalized/FeatureServer/0/query)

Let's try to build a request to retrieve the full dataset:

* The first parameter is `where`. It is obligatory and cannot be omitted. If there is no *where-clause* (i.e. if you want to extract the full dataset) you have to populate this parameter with the value `1=1` (which is - of course - always true...)
* To get all the attribute columns you have to give the parameter `outFields` the value `*`
* To retrieve the spatial data the parameter `returnGeomatry` should be set to `true`
* And the outputformat - `f` - is set to `geojson`


\newpage

\begin{figure}[H]
\centering
\caption{On this page the \textbf{Feature Layer} \textit{USA\_States\_Generalized} is presented in the \textbf{ArcGIS REST Services Directory}}
\vspace{5pt}
\includegraphics{Images/ArcGIS_REST_Services_Directory_USA_States_FeatureServer.png}
\label{USA_States_AG_REST_Services_Dir}
\end{figure}
\begin{figure}[H]
\centering
\caption{On this page you can enter parameters to query the \textbf{Feature Layer} \textit{USA\_States\_Generalized}}
\vspace{5pt}
\includegraphics{Images/ArcGIS_REST_Services_Directory_USA_States_FeatureServer_query.png}
\label{USA_States_AG_REST_Services_Dir_Query}
\end{figure}


\newpage

This is the script to populate the full request:

```{r}
# USA: An example with American data
library(httr)
library(sf)
library(tmap)

url <- parse_url("https://services.arcgis.com/P3ePLMYs2RVChkJx/arcgis/rest/services")
url$path <- paste(url$path, "USA_States_Generalized/FeatureServer/0/query", sep = "/")
url$query <- list(where = "1=1",
                  outFields = "*",
                  returnGeometry = "true",
                  f = "geojson")
request <- build_url(url)
```

The variable `request` will now contain this [link](https://services.arcgis.com/P3ePLMYs2RVChkJx/arcgis/rest/services/USA_States_Generalized/FeatureServer/0/query?where=1%3D1&outFields=%2A&returnGeometry=true&f=geojson). (When you click this link the server response will be shown in your browser in GeoJSON format.)

Now we want to feed this response directly into R, like this:

```{r}
USA_States_2017 <- st_read(request)
```

\newpage

Have a look at the result:

```{r}
qtm(USA_States_2017)
```



\tcbset{coltitle=black, colbacktitle=white, colframe=gray!75!black,colback=white,nobeforeafter}
\begin{tcolorbox}[enlarge by=5mm, hyphenationfix, title=\Large{Anchorage}]
"Hey Chel you know it's kinda funny\\
 Texas always seems so big\\
 But you know you're in the largest State in the Union\\
 When you're anchored down in Anchorage"\\
\\ 
 \textbf{Michelle Shocked} - Short Sharp Shocked - 1988
\end{tcolorbox}

\newpage

## Additional parameters to the `query` request

### Filter the records returned: specify the `where`-clause

To filter records you can specify a `where`-clause.

The `query` request below only returns those States which have more than 10 million inhabitants:

```{r}
# USA: An example with American data
library(httr)

url <- parse_url("https://services.arcgis.com/P3ePLMYs2RVChkJx/arcgis/rest/services")
url$path <- paste(url$path, "USA_States_Generalized/FeatureServer/0/query", sep = "/")
url$query <- list(where = "POPULATION>10000000",
                  outFields = "STATE_NAME,POPULATION",
                  returnGeometry = "true",
                  f = "geojson")
request <- build_url(url)
```

The variable `request` will now contain this [link](https://services.arcgis.com/P3ePLMYs2RVChkJx/arcgis/rest/services/USA_States_Generalized/FeatureServer/0/query?where=POPULATION%3E10000000&outFields=STATE_NAME%2CPOPULATION&returnGeometry=true&f=geojson). (When you click this link the server response will be shown in your browser in GeoJSON format.)


### Limit the number of columns returned: the `outFields` parameter

If you do not need all attribute columns of a dataset in your analysis, you can limit the number of columns returned by specifying the `outFields` parameter. You can specify a single attribute, or multiple attributes separated by commas.

The `query` request below only returns the geometry and the columns STATE_NAME and POPULATION for the United States:

```{r}
# USA: An example with American data
library(httr)

url <- parse_url("https://services.arcgis.com/P3ePLMYs2RVChkJx/arcgis/rest/services")
url$path <- paste(url$path, "USA_States_Generalized/FeatureServer/0/query", sep = "/")
url$query <- list(where = "1=1",
                  outFields = "STATE_NAME,POPULATION",
                  returnGeometry = "true",
                  f = "geojson")
request <- build_url(url)
```

The variable `request` will now contain this [link](https://services.arcgis.com/P3ePLMYs2RVChkJx/arcgis/rest/services/USA_States_Generalized/FeatureServer/0/query?where=1%3D1&outFields=STATE_NAME%2CPOPULATION&returnGeometry=true&f=geojson). (When you click this link the server response will be shown in your browser in GeoJSON format.)

\newpage

## Advanced topics

### Capabilities of the hosted feature service

So what exactly are the capabilities of a hosted feature service? To answer this question we can take a look at the information on the service in the ArcGIS REST Services Directory.     

Let's give it a try. We'll query the [USA Rail Road](https://www.arcgis.com/home/item.html?id=d209f26edc86485a9c631311e50d9940) dataset in ESRI's Living Atlas of the World. This is data from the Federal Railroad Administration (FRA) which is passed on by Esri.

The service URL is https://services.arcgis.com/P3ePLMYs2RVChkJx/arcgis/rest/services/USA_Railroads_1/FeatureServer  

A request to this URL will return the capabilities of the service:

```{r, message = FALSE, warning = FALSE}
library(httr)
library(jsonlite)
library(sf)
```
```{r}
url <- parse_url("https://services.arcgis.com/P3ePLMYs2RVChkJx/arcgis/rest/services")
url$path <- paste(url$path, "USA_Railroads_1/FeatureServer", sep = "/")
url$query <- list(f = "json")
request <- build_url(url)
response <- fromJSON(request) 
```

Notice that the format of the response is JSON instead of GeoJSON (query parameter `f = "json"`).   

The `response` is a list with all the capabilities. Take some time to have a look at it.

For example:
```{r}
response$description
response$copyrightText
```

\newpage

### Layers

A hosted feature service can have multiple layers. Every layer has a name and an id. The id is necessary for constructing the appropriate URL to query the features.    

The response contains the layer information needed:

```{r}
response$layers$name
```

The USA Rail Road hosted feature service only has one layer called *USA Railroads*. 

```{r}
library(dplyr)

layer_id <- response$layers %>% filter(name == 'USA Railroads') %>% select(id) 
layer_id
```

The id of the USA Railroads layer is `r layer_id`. We use this information to construct the correct value for the `path` variable.

```{r}
layer_path <- paste("USA_Railroads_1/FeatureServer", layer_id, sep = "/")
layer_path
```

### maxRecordCount

The response also contains other valuable information like the maximum number of records returned per request:

```{r}
response$maxRecordCount
```

```{r echo = FALSE, results = 'hide'}
maxRecordCount <- response$maxRecordCount
```

\newpage

### Capabilities of an individual layer

We can get even more information on the capabilities of the railroad layer:
```{r}
url <- parse_url("https://services.arcgis.com/P3ePLMYs2RVChkJx/arcgis/rest/services")
url$path <- paste(url$path, layer_path, sep = "/")
url$query <- list(f = "json")
request <- build_url(url)
response <- fromJSON(request) 
```
For example information on the fields in the dataset:
```{r}
response$fields %>% select(name, type)
```

### returnCountOnly

So now we know the query URL, the maximum number of records returned per request and the fieldnames in the dataset. 
We are ready to request the records!     

But wait, all the railroads in the United States of America, wouldn't that be *a lot* of records? Let's find out, before we fetch them all. We can do this by specifying the query parameter `returnCountOnly = "true"`:    

```{r}
url <- parse_url("https://services.arcgis.com/P3ePLMYs2RVChkJx/arcgis/rest/services")
url$path <- paste(url$path, layer_path, "query", sep = "/")
url$query <- list(where = "1=1",
                  returnCountOnly = "true",
                  f = "geojson")
request <- build_url(url)
response <- fromJSON(request) 
hits <- response$properties$count
hits
```

`r format(hits, big.mark = ",")` is way more that the maximum record count per request of `r format(maxRecordCount, big.mark = ",")`!

\newpage

### Automatic paging (by GDAL)

Let's do a request to fetch the records.

```{r}
url <- parse_url("https://services.arcgis.com/P3ePLMYs2RVChkJx/arcgis/rest/services")
url$path <- paste(url$path, layer_path, "query", sep = "/")
url$query <- list(where = "1=1",
                  outFields = "OBJECTID, NET_DESC",
                  returnGeometry = "true",
                  f = "geojson")
request <- build_url(url)
railroads <- st_read(request)
```

What is the number of records in the railroads dataframe? One would expect, not more than `r format(maxRecordCount, big.mark = ",")`.

```{r}
nrow(railroads)
```

Isn't that strange? The dataframe contains all `r format(hits, big.mark = ",")` records. How is this possible?    
Well the GDAL library makes it very easy for us. It detects that multiple requests are needed to fetch all the records and does this automagically for us. So no need to think of response paging, GDAL already has the solution for that!

A lot of the railroads are labelled 'abandonned'. They need to be filtered out. Of course you can do this *after* you fetched the records, but this isn't very efficient. Let's change the parameter value of `where` in our request, so only railroads in use are included in the response. 

```{r}
url <- parse_url("https://services.arcgis.com/P3ePLMYs2RVChkJx/arcgis/rest/services")
url$path <- paste(url$path, layer_path, "query", sep = "/")
url$query <- list(where = "NET_DESC <> 'Abandoned'",
                  outFields = "OBJECTID, NET_DESC",
                  returnGeometry = "true",
                  f = "geojson")
request <- build_url(url)
```
```{r}
railroads_in_use <- st_read(request)
nrow(railroads_in_use)
```

`r format(nrow(railroads_in_use), big.mark = ",")` is still a lot of records, but let's have a look at the result and add the railroads on top of a map of the USA:

```{r}
url <- parse_url("https://services.arcgis.com/P3ePLMYs2RVChkJx/arcgis/rest/services")
url$path <- paste(url$path, "USA_States_Generalized/FeatureServer/0/query", sep = "/")
url$query <- list(where = "1=1",
                  returnGeometry = "true",
                  f = "geojson")
request <- build_url(url)
USA_States_2017 <- st_read(request)
```

\newpage

When we plot the rail network we do see a clear concentration in the Eastern part of the continent:

```{r railroads_map, results = 'hide'}
library(tmap)

tm_shape(USA_States_2017) +
  tm_borders("black") +
  tm_shape(railroads_in_use) +
  tm_symbols(col = "red", scale = 0.1, border.lwd = NA) +
  tm_legend(show = FALSE) 
```

## Further reading

* More on the **ArcGIS REST API** can be found at the ArcGIS for Developers pages: [https://developers.arcgis.com/rest/](https://developers.arcgis.com/rest/)

\newpage

# Statistics Netherlands (CBS) StatLine databank as open data

\begin{multicols}{2}

All tables in the \href{https://opendata.cbs.nl/statline/portal.html?_la=en&_catalog=CBS}{Statistics Netherlands (CBS) StatLine databank} are available as open data. Since 2014 this databank has an open data web API based on the OData protocol (\url{https://www.odata.org/}).

Using web services, data can be retrieved, filtered and combined. In this way, Statistics Netherlands aims to promote the widespread use of its statistical data.

In this chapter we will load Dutch statistical data directly into R, using the package `cbsodataR`, the Statistics Netherlands (CBS) Open Data API Client for R.

\includegraphics{Images/cbs.png}

\end{multicols}

## the package `cbsodataR`

The package `cbsodataR` can be found on CRAN: [https://cran.r-project.org/package=cbsodataR](https://cran.r-project.org/package=cbsodataR)

```{r table_of_contents, echo=FALSE, results='hide'}
library(cbsodataR)

toc <- cbs_get_toc()
nrow(toc)
toc_nl <- cbs_get_toc(Language = "nl")
nrow(toc_nl)
toc_en <- cbs_get_toc(Language = "en")
nrow(toc_en)
```

The table of contents can be retrieved with the function `cbs_get_toc()`. As we browse through the table of contents we do see a total of `r nrow(toc)` entries. For most datasets all the metadata is in Dutch, but for some of them this information is also available in English.

When we split the TOC by language we do see that currently `r nrow(toc_en)` datasets are available with metadata in English. These 'English' datasets are actually copies of their Dutch equivalents. (Maybe overtime all tables will be available with a description in English?)

This is a wealth of information, and all these datasets can be directly loaded into R. In the next paragraph we will do some exercises with one particular table.

```{r ref.label = 'table_of_contents'}
```

\newpage

## Exercises with regional statistical data

### Preparing the data

The table we have chosen for the exercises in this paragraph is called 'Regionale kerncijfers Nederland', i.e. regional statistical data about the Netherlands.
The identifier of this table is **70072ned**. It is a huge table containing data at 5 regional levels - from the country as a whole down to the municipal level - from 1995 up to the current year.

The table can be retrieved with the function `cbs_get_data()`:

```{r eval = FALSE}
NL_Regional_Statistics2017 <- cbs_get_data('70072ned')
```

As we do not need the full table, we will limit the amount of data loaded.

Firstly, we are only interested in the year 2017. We want to analyse the total population by region for that particular year.

So, we come up with the following statement (we will explain the selection of columns below):

```{r, message = FALSE, warning = FALSE}
library(dplyr)
library(stringr)
```

```{r}
NL_Regional_Statistics2017 <- cbs_get_data('70072ned', Perioden = "2017JJ00") %>% 
  select(RegioS, Perioden, TotaleBevolking_1, Code_291, Naam_292, Code_293, Naam_294)
```

The first two columns - `RegioS` and `Perioden` - are categorical columns, i.e. they contain codes. The labels for these columns can be added with the function `cbs_add_label_columns()`:

```{r}
NL_Regional_Statistics2017 <- cbs_add_label_columns(NL_Regional_Statistics2017)
```

We will now rename the columns of our table for further use in the exercise:

```{r}
NL_Regional_Statistics2017 <- rename(NL_Regional_Statistics2017, 
                                     Code = RegioS, 
                                     Name = RegioS_label,
                                     Year = Perioden,
                                     Year_label = Perioden_label,
                                     Total_population = TotaleBevolking_1,
                                     Region_code = Code_291,
                                     Region_name = Naam_292,
                                     Province_code = Code_293,
                                     Province_name = Naam_294)
```
Each column has a label. To avoid issues later on in this exercise, we will remove these labels now:

```{r}
cols <- colnames(NL_Regional_Statistics2017)
for (c in cols) {
  attr(NL_Regional_Statistics2017[[c]], "label") <- NULL
}
```
\newpage

Let's have a first look at the table:

```{r}
head(NL_Regional_Statistics2017)
```

In the first row we can see the total number of inhabitants in the Netherlands on January 1st, 2017, which is: **17,081,507**.

And wait, we notice something else: some columns seem to have a fixed width. For example, the columns `Code` and  `Region_name`:

```{r}
paste(NL_Regional_Statistics2017$Code[1])
paste(NL_Regional_Statistics2017$Region_name[6])
```
```{r}
my_string <- paste(NL_Regional_Statistics2017$Region_name[6])
nchar(my_string)
```
This is weird, isn't it? The string 'Noord-Nederland' - which is only 15 charaters in length - is filled out with trailing spaces to have a length of 50... This can't be good, can it? We want our strings to behave like strings with their proper length, not carrying around a trail of whitespaces.

Let's fix this issue with the function `trimws()` for the columns concerned:

```{r}
cols <- c('Code', 'Region_code', 'Region_name', 'Province_code', 'Province_name')
NL_Regional_Statistics2017[cols] <- 
  lapply(NL_Regional_Statistics2017[cols], function(x){as.factor(trimws(x))})
```

```{r}
head(NL_Regional_Statistics2017)
```

This looks much better!

\newpage

\tcbset{coltitle=black, colbacktitle=white, colframe=gray!75!black,colback=white,nobeforeafter}
\begin{tcolorbox}[enlarge by=5mm, hyphenationfix, title=\Large{NUTS - Nomenclature des UnitÈs Territoriales Statistiques}]
\href{http://ec.europa.eu/eurostat/web/main/home}{Eurostat} uses a set of geospatial classifications, but the heart of these is NUTS:  the \textbf{Nomenclature of Territorial Units for Statistics}.\\
This classification was set up by Eurostat at the beginning of the 1970s. It serves as a single, coherent system for dividing up the EU's territory in order to produce regional statistics for the European Union.\\
\\
Ther are three levels of NUTS regions:
\begin{itemize}
\item{NUTS 1: major socio-economic regions}
\item{NUTS 2: basic regions for the application of regional policies}
\item{NUTS 3: small regions for specific diagnoses}
\end{itemize}

\includegraphics{Images/Eurostat_logo.png}
More information on the NUTS classification can be found here: \url{http://ec.europa.eu/eurostat/web/nuts/background}\\
\\
Eurostat even created a nice video on the topic: \url{https://youtu.be/a4Y-hCQ-Klo}
\end{tcolorbox}

### Extracting NUTS 1 and NUTS 2 regions

Now that we have prepared our data, we can extract the data about NUTS 1 and NUTs 2 respectively. At NUTS 1 level the Netherlands is divided into 4 regions ('Landsdelen') and at NUTS 2 level into 12 provinces:

```{r}
library(stringr)

NL_Regions2017_data <- filter(NL_Regional_Statistics2017, str_detect(Code, "LD"))
NL_Provinces2017_data <- filter(NL_Regional_Statistics2017, str_detect(Code, "PV"))
```

Please note: these new data frames inherit factor levels from the original data frame from which they are created. Use the **RStudio Environment pane**, to confirm that - for example - the `Name` column of the `NL_Provinces2017_data` table is now a Factor with 770 levels, whereas there are only 12 provinces. So you have to drop unused levels before you can use the new subsets for further analysis:

```{r}
# Make sure to drop unused levels from the factors in those new data.frames
NL_Regions2017_data <- droplevels(NL_Regions2017_data)
NL_Provinces2017_data <- droplevels(NL_Provinces2017_data)
```

\newpage

```{r}
NL_Provinces2017_data %>% 
  select(Code, Name, Total_population, Region_code, Region_name)
```

We have noticed that the province names in the column `Name` all have the suffix *(PV)*. Let's get rid of this suffix, while making sure that the column remains a factor:

```{r}
NL_Provinces2017_data$Name <- 
  as.factor(str_replace(NL_Provinces2017_data$Name," \\(PV\\)", ""))
```

Likewise, the region names in the column `Name` all have the suffix *(LD)*. We will also remove that:

````{r}
NL_Regions2017_data %>% select(Code, Name, Total_population)
NL_Regions2017_data$Name <- 
  as.factor(str_replace(NL_Regions2017_data$Name," \\(LD\\)", ""))
```

\newpage

### `barplot()`

Let's investigate the spread of the Dutch population over the different regions by creating a barplot.

The most basic call to the `barplot()` function, is just to provide a column from a dataset and leave all the defaults:

```{r exbplot1, eval= FALSE}
barplot(NL_Provinces2017_data$Total_population) 
# Try this yourself - result not printed in this manual
```

This returns a very basic plot, with no labels and no title, so not very useful.

Use `?barplot()` to see what arguments we can use. After some trial and error we might come to a statement like this:

```{r exbplot2}
barplot(NL_Provinces2017_data$Total_population / 1000000, 
        names = NL_Provinces2017_data$Name, 
        las = 2, cex.axis = .6, cex.names = .6, 
        cex.main = .8, cex.lab = .8, space = 0, 
        col = "lightblue", 
        ylab = "Inhabitants (* 1.000.000)", 
        main = "Number of Inhabitants by Province - The Netherlands - 2017")
```

Not bad at all, for a first attempt. Please note: the provinces are not presented in alphabetical order, but in their  *natural* order, from Groningen (PV20) in the North to Limburg (PV31) in the South.

The provinces at the NUTS 2 level are grouped into regions ('Landsdelen') at the NUTS 1 level. We do know for each province to which region it belongs. Wouldn't it be nice to reflect this division in the plot?

So, In the next plot we will color the bars by region - North, East, West, South - using the `col` argument.

We will need 4 different colors for this plot. In the statement below we use the `palette()` function to manipulate the color palette which is used when a `col=` has a numeric index. Using the `c()` function you create a vector with 4 distinct colors, which is passed as an argument into `palette()`:

```{r}
palette(c("royalblue3", "firebrick3", "darkolivegreen4", "goldenrod1"))
```

And now we can create the barplot like this - with the arguments for the legend provided as a `list`:

```{r}
barplot(NL_Provinces2017_data$Total_population / 1000000, 
        names = NL_Provinces2017_data$Name, 
        las = 2, cex.axis = .6, cex.names = .6, 
        cex.main = .8, border = "grey", 
        col = NL_Provinces2017_data$Region_code, 
        ylab = "Inhabitants ( * 1,000,000)", 
        main = "Number of Inhabitants by Province - The Netherlands - 2017", 
        legend.text = unique(NL_Provinces2017_data$Region_name), 
        args.legend = list(x = 'topleft', 
                           bty = 'n', 
                           fill = unique(NL_Provinces2017_data$Region_name), 
                           border = 'grey'))
```


\newpage

### `pie()`

Based on the plots in the previous sections we might think that almost half the Dutch population lives in the westernmost region.
  
To confirm this hypothesis we will create a pie chart with the population by region.

This basic statement will create a simple pie chart:
```{r}
pie(NL_Regions2017_data$Total_population, labels = NL_Regions2017_data$Name)
```
And yes, almost half of the Dutch population lives in West-Nederland.

\newpage

The pie chart on the previous page already confirmed what we wanted to know. But of course - with a little extra effort - we can improve the quality of the graph by adding a title, calculating the actual percentages and using the same colors for the regions as we did in the barplot:

```{r}
pct <- paste0(round(NL_Regions2017_data$Total_population / 
                      sum(NL_Regions2017_data$Total_population) * 100, 1), "%")
lbls <- paste(NL_Regions2017_data$Name, "\n", pct)
palette(c("royalblue3", "firebrick3", "darkolivegreen4", "goldenrod1"))
pie(NL_Regions2017_data$Total_population, labels = lbls, clockwise = TRUE,
    cex = .8, col = NL_Regions2017_data$Name, border = "grey",
    main = "Percentage of Inhabitants by Region - The Netherlands - 2017")
```

```{r echo = FALSE, results = 'hide', message = FALSE}
palette("default")
```

\newpage

### Extracting municipal data


Now we will extract the municipalities from `NL_Regional_Statistics2017`.

```{r}
NL_Municipalities2017_data <- filter(NL_Regional_Statistics2017, str_detect(Code, "GM"))
nrow(NL_Municipalities2017_data)
```
Are there `r nrow(NL_Municipalities2017_data)` municipalities in the Netherlands? No, not anymore!
There used to be much more: in 1850 there were more or less 1200 municipalities. But today there are much less: the current (2018) amount is 380. The Netherlands has a long standing tradition of grouping smaller municipalities into larger ones. This is a slow process, but each year on the 1^st of January the number of municipalities diminishes again.

We are looking at 2017, and in that year there were still 388 municipalities.

The table we have accessed contains data from 1995 up to today. And apparently in that year there were still 713 municipalities.

We can filter out the municipalities that do no longer exist by removing the ones without data. Here we use the column `Total_population` to check for that:

```{r}
NL_Municipalities2017_data <- filter(NL_Municipalities2017_data, Total_population != "")
nrow(NL_Municipalities2017_data)
```

And of course we should also drop unused factor levels:
```{r}
NL_Municipalities2017_data <- droplevels(NL_Municipalities2017_data)
```

\newpage

### Merging sf and data.frame objects

Now we want to plot a map with the regional subdivision. To be able to do so we have to merge our data frame with a dataset containing geometry.

We will access a WFS server to download the municipal geometry - for the year 2017! - as we have done in paragraph \ref{GFnl}:

```{r}
# NL: Example with Dutch data
library(httr)
library(sf)
library(tmap)

url <- parse_url("https://geodata.nationaalgeoregister.nl/cbsgebiedsindelingen/wfs")
url$query <- list(service = "WFS",
                  version = "2.0.0",
                  request = "GetFeature",
                  typename = "cbsgebiedsindelingen:cbs_gemeente_2017_gegeneraliseerd",
                  outputFormat = "application/json")
request <- build_url(url)

NL_Municipalities2017 <- st_read(request)
```

As we do not need all attribute columns, we will select only the relevant ones:

```{r}
NL_Municipalities2017 <- select(NL_Municipalities2017, statcode, statnaam)
```

Please note: in the statement above we only select 2 columns, but the resulting object does also contain the `geometry`column. This is due to the *sticky* character of this `geometry` column. When you select a subset of columns, the geometry will always be included, unless you explicitly drop it. In other words: a subset of an `sf` object will in itself also be an `sf` object.

Now we will rename the columns:

```{r}
NL_Municipalities2017 <- rename(NL_Municipalities2017, Code = statcode, Name2 = statnaam)
```

Both datasets do contain a column with municipal codes, so now you can merge them, like you would do with any two data frames:

```{r}
NL_Municipalities2017 <- 
  merge(NL_Municipalities2017, NL_Municipalities2017_data, by = "Code")

class(NL_Municipalities2017)
```
\newpage

### `qtm()`

And now we can plot our map with NUTS 1 regions, again using the same colors as before:
```{r echo=FALSE}
palette(c("royalblue3", "firebrick3", "darkolivegreen4", "goldenrod1"))
```
```{r}
qtm(shp = NL_Municipalities2017,
    title = "Regional subdivison",
    fill = "Region_name",
    fill.title = "The Netherlands - 2017\nRegion",
    fill.palette = 
      palette(c("royalblue3", "firebrick3", "darkolivegreen4", "goldenrod1")),
    borders = "grey",
    format = "NLD_wide")
```

\newpage

\tcbset{coltitle=black, colbacktitle=white, colframe=gray!75!black,colback=white,nobeforeafter}
\begin{tcolorbox}[enlarge by=5mm, hyphenationfix, title=\Large{Factors in R}]
Conceptually, factors are variables in R which can only contain a pre-defined set of values, known as levels. Factors are often referred to as \textit{categorical variables}, which can either be \textbf{ordered} (e.g. 'low', 'medium', 'high') or \textbf{unordered} (e.g. 'male', 'female').\\
\\
One of the most important uses of factors is in statistical analysis and plotting. Storing categorical variables as factors insures that the statistical modeling functions will treat such data correctly. Factors are stored in R as a vector of integers, with a corresponding set of labels. These labels - one for each level - are used when the factor is displayed.\\
\\
When data are being loaded into R, for example with the function \textbf{read.csv()}, all columns containing text (character data) will be automatically converted to factors. While this is very useful in many cases, there might also be situations where you will want to treat your data as continuous variables.
\end{tcolorbox}

## Factor: group municipalities into categories \label{parFactor}

In a previous exercise we have learned that there is an ongoing process of municipal regrouping in the Netherlands. Let's investigate opportunities for future regroupings by maping municipalities by size.

We will start this exercise by dividing our municipalities in 3 groups: small, medium and large.

A small municipality has less than 50,000 inhabitants, a medium sized one has between 50,000 and 200,000, whereas a municipality with more than 200,000 inhabitants is considered to be large.


```{r ordered_factor}
NL_Municipalities2017 <- mutate(NL_Municipalities2017,
  Category = case_when(Total_population < 50000 ~ "Small population",
                       Total_population >= 50000 & 
                       Total_population < 200000 ~ "Medium population",
                       Total_population >= 200000 ~ "Large population"))
```

```{r ordered_factor2}
class(NL_Municipalities2017$Category)
```

Now we will create an ordered factor with 3 levels:

```{r}
population_levels <- c("Small population", "Medium population", "Large population")
NL_Municipalities2017 <- mutate(NL_Municipalities2017, 
  Category = factor(Category, levels = population_levels, ordered = TRUE))

class(NL_Municipalities2017$Category)
```
```{r change_categories, echo = FALSE, results = 'hide', message = FALSE}
population_levels <- c("Very small population", "Small population", "Medium population", "Large population")
NL_Municipalities2017 <- mutate(NL_Municipalities2017, 
  Category = case_when(Total_population < 20000 ~ "Very small population", TRUE ~ as.character(Category)) %>% 
    factor(levels = population_levels, ordered = TRUE))
```

\newpage

And now we can plot our map.

Please note: in the map printed below there are not 3 categories, but 4. We have added a category 'very small' for municipalities with less than 20,000 inhabitants.

Question:

What steps do you need to take to add a new factor level after factors have been defined?

As you can see there is still some potential to cluster very small municipalities together. But of course, this will be more a political than a scientific issue.


```{r tmapcategories}
qtm(shp = NL_Municipalities2017, title = "Municipalities by Population Size",
    fill = "Category", fill.title = "The Netherlands - 2017\nCategory", 
    borders = "grey", format = "NLD_wide")
```

\newpage

# Spatial Reference Systems \label{ChapCRS}

\begin{multicols}{2}

In this chapter we will have a quick look at an important topic when handling geospatial data, i.e the Coordinate Reference System (or Spatial Reference System), which defines how spatial elements relate to the surface of the Earth and which is used to project our geodata on a flat screen. We have ignored this subject more or less in the previous exercises, mainly because of the fact that the whole matter of projecting the data is handled very well in the `sf` package, or rather the `proj.4` software behind it. We just didn't have to think about it. Or wait, we did set a `crs` explicitly - using an `epsg` code - in the very first exercise this morning. Remember?

In general when you read in spatial data it comes with a proper CRS and if all your data is in this same projection, you really don't have to worry about all the geodesy behind it. But what if you want to combine data from different sources which appear to have different spatial reference systems?

Therefor we will shortly touch upon:

\begin{itemize}
\item{Geographic coordinate systems vs. Projected coordinate systems}
\item{the two main ways to describe a CRS in R: the `epsg` code and the `proj4string` definition}
\item{how to reproject your data - temporarily or permanently}
\end{itemize}

\end{multicols}

## Coordinate systems: Geographic vs. Projected

Geographic coordinate systems identify locations on the Earth's surface using longitude and latitude. Longitude is the distance East or West in angular distance, i.e. measured in degrees, from the Prime Meridian plane. Latitude is this distance North or South of the equatorial plane. And no, there is not just one 'Lat\\Lon coordinate system'. All geographic systems have to use the same reference, i.e. the Prime Meridian and the Equator, but the exact location on the Earth depends on several parameters like the ellipsoid and the datum used. So even if someone gives you data 'in degrees' you still have to find out which CRS to use. With a wrong CRS your data might end up centimeters or even several meters from the correct location. An important advantage of these geographic systems is their global reach - they can be used everywhere and are useful when dealing with international datasets. An important disadvantage is: distances cannot be easily measured in for example meters or kilometers in geographic CRSs.

Projected coordinate system have a much smaller reach, as these are based on Cartesian coordinates on an implicitly flat surface. They are only valid for a specific area - a country or even just a part of a country - where they have an origin, x and y axes, and a linear unit of measurement such as meters. We have seen several examples of data in such local, projected systems, e.g. EPSG:31287 for Austria, EPSG:3067 for Finland and EPSG:28992 for the Netherlands (but only onshore). These systems are ideal for dealing with data at a national or subnational level. In such a case you shuld really stick with the local CRS. 

## `epsg` codes and. `proj4string` definitions

As there are so many coordinate systems worldwide it is good to be clear about what system you use in your analysis. There are two main ways to describe a CRS in R: the `epsg` code and the `proj4string` definition. If you get your spatial data from a proper source, the `st_read()` function will return both, as we have seen in the exercises before.

The `epsg` code gets it name from the - no longer existing - *European Petroleum Survey Group*. The set of EPSG definitions is currently maintained by the  International Association of Oil & Gas Producers (IOGP). Apparently location is very important in the fossil fuel industry.

An `epsg` code refers to only one, well-defined coordinate reference system. This coding system is widely adopted by the geospatial community, so if you get XY data with an existing `epsg` code you can easily plot your data on the map, in R or in any other GIS system for that matter.

A `proj4string` definition contains different parameters such as the projection type, the datum and the ellipsoid. It allows you to specify your own projection, or to modfy an existing one. But in general it would be advisable to refer to the data provider if there is something wrong with the projection of your data.

\newpage

## Reprojecting data

```{r}
NL_Airports <- st_read("NL_Airports.geojson")
```

```{r}
NL_Airports <- st_join(NL_Airports, NL_Municipalities2017)
```

```{r}
NL_Airports <- st_join(st_transform(NL_Airports, 28992), NL_Municipalities2017[,c("Name","Province_name","Region_name")])
NL_Airports
```

```{r echo = FALSE}
# Delete the output files to keep the repository clean
unlink("NL_Airports.geojson")
```


## Further reading

A good introduction into Spatial Refernce Systems in R (and many other topics R spatial):

* **Lovelace, Robin, Jakub Nowosad & Jannes Muenchow** (Forthcoming). *Geocomputation with R*. CRC Press. Online version: [https://geocompr.robinlovelace.net/](https://geocompr.robinlovelace.net/)



\newpage

# Manipulating Spatial Data \label{ChapMan}

## Reading the datasets

\begin{multicols}{2}

The municipality of The Hague publishes some nice datasets on \url{http://denhaag.dataplatform.nl}. We'll use some of them to learn more about manipulating spatial data:

\begin{itemize}
\item{Neighborhoods (polygons)}
\item{Trees (points)}
\item{Bat flight paths (lines)}
\end{itemize}

\includegraphics[width=80mm]{Images/DH-NL-Rgb-CS6.png}

\end{multicols}

```{r}
library(sf)
library(dplyr)

# Neighborhoods in The Hague
url <- "https://ckan.dataplatform.nl/dataset/c1059cef-be66-4a7a-9657-2f38f55794ed/resource/a175afe5-67e2-4e45-8b71-62f30377bf7d/download/wijken.json"
neighborhoods <- st_read(url)
```

Please note: the Spatial Reference System of this dataset is WGS84, i.e. a geographic coordinate system. In chapter \ref{ChapCRS} we have learned that if you want to make measurements, you should use a projected  coordinate system.

Therefor we reproject the dataset to RD_New (RD_New or EPSG:28992 is the projected coordinate system for The Netherlands):

```{r}
neighborhoods <- st_transform(neighborhoods, 28992)
```


\newpage

```{r}
# Trees in The Hague
url <- "https://ckan.dataplatform.nl/dataset/d604d9bb-8c2f-4e7d-a69c-ee6102890baf/resource/85327fde-9e76-40f3-a8d4-25970896fd8f/download/bomen-json.zip"
zip_file <- tempfile(fileext = ".zip")
download.file(url, destfile = zip_file, mode = "wb")
dir.create("./Data", showWarnings = FALSE)
unzip(zip_file, exdir = "./Data")
unlink(zip_file)
rm(url, zip_file)
trees <- st_read("Data/bomen-json.json") %>%
         select(id = ID, species = BOOMSOORT_WETENSCHAPPELIJ, age = LEEFTIJD)
```

\newpage

```{r}
# Bat flight routes in The Hague
url <- "https://ckan.dataplatform.nl/dataset/c7e9cb41-3b2d-47a4-9f1e-60ee7708f561/resource/ed7d5778-c890-4f4e-bfc3-048923761ace/download/vleermuisroutes.json"
bat_flight_paths <- st_read(url) %>% select(func = FUNCTIE, id = COUNTER)
```

```{r}
levels_dutch <- c("Migratieroute water- en meervleermu", 
                  "Vliegroute gewone dwergvleermuis", 
                  "Vliegroute laatvlieger", 
                  "Vliegroute rosse vleermuis", 
                  "Vliegroute watervleermuis")
levels_eng <- c("Migration route Myotis daubentonii and Myotis dasycneme", 
                "Flight path Pipistrelllus pipistrellus", 
                "Flight path Eptesicus serotinus", 
                "Flight path Nyctalus noctula", 
                "Flight path Myotis daubentoni")
bat_flight_paths$func <- 
  plyr::mapvalues(bat_flight_paths$func, from = levels_dutch, to = levels_eng)
``` 

We will also reproject this dataset:

```{r}
bat_flight_paths <- st_transform(bat_flight_paths, 28992)
```

\newpage

## `st_area()`

When we have a look at the dataset `neighborhoods` in the RStudio Data Viewer (`View(neighborhoods)`) we will notice a column called OPPERVLAKTE, meaning 'AREA'. This is a numeric field:

```{r}
class(neighborhoods$OPPERVLAKTE)
```
**Please note:** it is bad habbit to store the AREA of a polygon in a permanent attribute field. Why? When a polygon is edited, the actual area of the object will change, but this static attribute value will remain the same.

So, it is best to only retrieve the area of our polygons when we actually need them in our analysis. To do so we can use the function `st_area()`:

```{r}
neighborhoods$area <- st_area(neighborhoods)
```

This is a 'units' field, with the units being square meters (the units are set according to the CRS)

```{r}
class(neighborhoods$area)
```

In this case, when we compare the content of the column `OPPERVLAKTE` with that of our newly created column `area` the values are still the same:

```{r}
neighborhoods[,c("OPPERVLAKTE","area")]
```

Now, let's rename our columns to English equivalents for further analysis (dropping this silly OPPERVLAKTE and some other irrelevant fields on the fly):

```{r}
neighborhoods <- neighborhoods %>%
                 select(nh_code = WIJKCODE, nh_name = WIJKNAAM, district_code = STADSDEELCODE)
```

\newpage

## The package `units` with the function `set_units()`

Here we would like to introduce the package `units` - also created by Edzer Pebesma - which provides support for measurement units in R.

**The package `units` on CRAN:**  
[https://cran.r-project.org/package=units](https://cran.r-project.org/package=units)

As we have seen in the previous paragraph - when we retreive the area of a polygon with the function `st_area()` the units will be taken from the CRS:

```{r}
neighborhoods$area <- st_area(neighborhoods)
```

But we think it a bit strange to talk about neighborhood size in terms of square meters. Don't you agree? We would rather know these values in square kilometers. Let's use the function `set_units()` to convert the values:

```{r}
library(units)
neighborhoods$area2 <- set_units(neighborhoods$area, km^2)
```

And we may also want to round these values to 2 digits:

```{r}
neighborhoods$area3 <- round(neighborhoods$area2, digits = 2)
```

And now it is simple to answer the following question: What is the largest neighborhood in The Hague?

```{r}
neighborhoods %>% filter(area == max(area)) %>% st_set_geometry(NULL)
```

## `st_length()`

In a similar way we can retrieve the length of line objects:

```{r}
bat_flight_paths$length <- st_length(bat_flight_paths)
```

Next question: What are the five shortest bat flight routes?

```{r}
bat_flight_paths %>% arrange(length) %>% 
                     st_set_geometry(NULL) %>% 
                     head(n = 5)
```
\newpage

## `st_distance()`

How many meters are tree 1351893 and 1398051 apart from each other?

```{r}
tree_1351893 <- filter(trees, id == 1351893)
tree_1398051 <- filter(trees, id == 1398051)

st_distance(tree_1351893, tree_1398051) %>% round()

```

How far is the nearest bat flight path from tree 1351893?

```{r}
st_distance(tree_1351893, bat_flight_paths) %>% min() %>% round()
```

And how many trees are there within 20 meters from the bat flight path 1509?
  
```{r}
flight_path_1509 <- filter(bat_flight_paths, id == 1509)
lengths(st_is_within_distance(flight_path_1509, trees, dist = 20))
```

\newpage
## Aggregating and filtering data

One of the nice things of sf objects is that you can take advantage of the dplyr functions like `select()`, `filter()`, `mutate()`, `group_by()`, `summarize()` and `arrange()`. We already saw some examples in earlier chapters.    
To illustrate the power of dplyr, let's calculate the count and mean age for each species in the trees dataset? List the species in alphabetical order. Only print the first 3 species.

```{r}
trees %>% select(species, age) %>% 
          group_by(species) %>% 
          summarize(mean_age = round(mean(age, na.rm =  TRUE), 2), count = n()) %>% 
          arrange(species) %>%
          st_set_geometry(NULL) %>% 
          head(n = 3)
```

## Spatial aggregation

You can also aggregate spatial data, for instance calculate the city border using the neighborhoods dataset:

```{r}
the_hague <- st_union(neighborhoods)
```

Or you can group the neighborhoods by district:

```{r}
the_hague_districts <- neighborhoods %>% group_by(district_code) %>% summarise()
```

```{r}
tm_shape(neighborhoods) +
  tm_borders(col = "grey", alpha = 0.5) +
tm_shape(the_hague_districts) +
  tm_borders(col = "blue", lwd = 2, alpha = 0.5) +
tm_shape(the_hague) +
  tm_borders("red", lwd = 3) +
tm_layout(frame = FALSE)
```

Or calculate the total number of trees for each neighborhood:

```{r warning = FALSE}
neighborhoods$total_trees <- lengths(st_covers(neighborhoods, trees))
```

It's always a good idea the verify your outcomes.

```{r}
nrow(trees)
sum(neighborhoods$total_trees)
```

We're short `r nrow(trees) - sum(neighborhoods$total_trees)` trees in the neighborhood dataset! Where did those trees go?

```{r message = FALSE}
trees_not_in_neighborhood <- trees[the_hague, op = st_disjoint]
nrow(trees_not_in_neighborhood)
``` 

`r nrow(trees_not_in_neighborhood)` trees are not in a neighborhood of The Hague. Let's make a plot:

```{r message = FALSE}
tm_shape(neighborhoods) +
  tm_borders("black") +
tm_shape(trees) +
  tm_symbols(col = "darkgreen", scale = 0.05, border.lwd = NA) +
tm_shape(trees_not_in_neighborhood) +
  tm_symbols(col = "red", scale = 0.05, border.lwd = NA) +
tm_layout(frame = FALSE) + 
tm_legend(show = FALSE)
```

To be on the safe side, we'll remove all the trees from our dataset that are outside the neighborhoods. These are the red trees in the plot.

```{r message = FALSE, warning = FALSE}
trees <- trees[the_hague, op = st_intersects]
nrow(trees)
```

\newpage

Another example of applying a spatial filter: Where are the bat flight paths in the Zorgvliet neighborhood?

```{r message = FALSE, warning = FALSE}
zorgvliet <- filter(neighborhoods, nh_name == "Zorgvliet")
bat_flight_paths_zorgvliet <- bat_flight_paths[zorgvliet, op = st_intersects]

tm_shape(zorgvliet) +
  tm_borders("black") +
tm_shape(bat_flight_paths_zorgvliet) +
  tm_lines(col = "blue") +
tm_layout(frame = FALSE) +  
tm_legend(show = FALSE)
```

That's kind of ugly, those flight paths running outside the neighborhood. Let's cut them on the neighborhood border. Also add the trees to the plot.

```{r message = FALSE, warning = FALSE}
bat_flight_paths_zorgvliet <- st_intersection(bat_flight_paths, zorgvliet)
trees_zorgvliet <- st_intersection(trees, zorgvliet)

tm_shape(zorgvliet) +
  tm_borders("black") +
tm_shape(trees_zorgvliet) +
  tm_symbols(col = "darkgreen", scale = 0.05, border.lwd = NA) +
tm_shape(bat_flight_paths_zorgvliet) +
  tm_lines(col = "blue") +
tm_layout(frame = FALSE) +
tm_legend(show = FALSE)
```
\newpage

## Calculating and visualizing density 

Note that the dataset only contains trees in public space. But still, the tree density may give us an idea of how 'green' a neighborhood is.   

What are the neighborhoods with the least and the most trees?

```{r}
neighborhoods$nh_name[which.min(neighborhoods$total_trees)] %>% as.character()
neighborhoods$nh_name[which.max(neighborhoods$total_trees)] %>% as.character()
```

`r neighborhoods$name[which.min(neighborhoods$total_trees)] %>% as.character()` is a neighborhood with no trees at all in public space. Let's find out where it is.

```{r message = FALSE}
no_trees <- filter(neighborhoods, total_trees == 0)

tm_shape(neighborhoods) +
  tm_borders("black") +
tm_shape(no_trees) +
  tm_fill("grey") +
tm_shape(trees) +
  tm_symbols(col = "darkgreen", scale = 0.05, border.lwd = NA) +
tm_layout(frame = FALSE) + 
tm_legend(show = FALSE)
```

Now visualize the tree density for each neighborhood in The Hague.

```{r}
neighborhoods$tree_density <- 
  round(neighborhoods$total_trees / units::set_units(neighborhoods$area, km^2))
qtm(shp = neighborhoods, fill = "tree_density", 
fill.palette = "Greens",
fill.style = "kmeans", title = "Tree density",
fill.title = parse(text = "trees/km^2"))
```

Again notice that this plot gives a distorted view of reality, because the dataset only contains trees in public space.But as an example of the power of spatial data manipulation and visualisation, it suffices.

\newpage

## Spatial joins

```{r echo = FALSE}
# Delete the Data directory to keep the repository clean
unlink("./Data", recursive = TRUE)
```

\newpage

# Geocoding

Often we will have a spreadsheet with addresses, but no coordinates linked to them. Before we can do any spatial analysis, we will have to _geocode_ the addresses.

## Geocoding with OpenStreetMap and Nominatim

We can geocode addresses using Nominatim, a geocoding API for OpenStreetMap.

```{r message = FALSE}
library(dplyr)
library(httr)
library(jsonlite)
library(sf)

url <- parse_url("http://nominatim.openstreetmap.org")
url$path <- paste("search", URLencode("Martinikerkhof 3 Groningen"), sep = "/")
url$query <- list(format = "json", 
                  adressdetails = 0, 
                  limit = 1)
request <- build_url(url)
result <- fromJSON(request) %>% 
          mutate(lat = as.numeric(lat),lon = as.numeric(lon)) %>%
          select(lat, lon, display_name)
result
```

Creating a `sf` object is easy now.

```{r}
martini_tower <- st_as_sf(result, coords = c("lon", "lat"), crs = 4326)
martini_tower
```

Now we can put the Martini Toren on the map!

```{r eval=FALSE}
library(tmap)

tmap_mode('view')
tm_shape(martini_tower) + tm_markers() +
  tm_legend(show = FALSE) + tm_view(basemaps = 'OpenStreetMap')
```

\includegraphics{Images/martini_tower.png}

Let's do some batch geocoding.

```{r}
url <- parse_url("http://nominatim.openstreetmap.org")
url$query <- list(format = "json", 
                  adressdetails = 0, 
                  limit = 1)

geocodeAddress <- function(x){
  url$path <- paste("search", URLencode(x[2]), sep = "/")
  request <- build_url(url)
  result <- fromJSON(request)
  data.frame(name = x[1], address = result$display_name, 
            lat = as.numeric(result$lat), lon = as.numeric(result$lon),
            row.names = x[1], stringsAsFactors = FALSE)
}

name <- c("Martinitoren", "Sint-Janskerk", "Onze-Lieve-Vrouwekathedraal")
address <- c("Martinikerkhof 3 Groningen", "Vrijthof 24 Maastricht", "Groenplaats 21 Antwerpen")
poi <- data.frame(name, address, stringsAsFactors = FALSE)

poi <- apply(poi, 1, geocodeAddress) %>% 
       bind_rows() %>% 
       st_as_sf(coords = c("lon","lat"), crs = 4326)
poi
```

## Geocoding with the BAG and Locatieserver (only for The Netherlands)

The Dutch National Spatial Data Infrastructure ([PDOK](http://www.pdok.nl)) is a central facility for unlocking geospatial open data of national importance. This is actual and reliable information for both the public and private sector.    
PDOK provides API's, for instance [Locatieserver](https://github.com/PDOK/locatieserver). Locatieserver is a very powerfull geocoding service. Upon request it will return the location of an adress in The Netherlands.
There doesn't necessarily has to be a 100% match with the names and adresses in the PDOK dataset. Every search result will have a score indicating the level of success. 

```{r message = FALSE}
library(dplyr)
library(httr)
library(jsonlite)
library(sf)

url <- parse_url("https://geodata.nationaalgeoregister.nl")
url$path <- "locatieserver/v3/free"
url$query <- list(q = "'Martinikerkhof 3 Groningen' and type:adres", rows = 1)
request <- build_url(url)
result <- fromJSON(request) 
martini_tower <- data.frame(addres = result$response$docs$weergavenaam, 
                            wkt = result$response$docs$centroide_rd) %>% 
                 st_as_sf(wkt = 2, crs = 28992)
martini_tower
```

In the previous example we selected the response attribute `centroide_rd`. This attribute contains the location in Well-Known Text (WKT) format and coordinate reference system RD_New (EPSG:28992).   
If we want the coordinates to be in WGS84 (EPSG:4326), we can select `centroide_ll`. 

Of course, you can also use Locatieserver for batch geocoding:

```{r}
url <- parse_url("https://geodata.nationaalgeoregister.nl")
url$path <- "locatieserver/v3/free"
url$query <- list(rows = 1)

geocodeAddress <- function(x){
  url$query$q <- paste("'", x[2], "'", "and type:adres")
  request <- build_url(url)
  result <- fromJSON(request)
  data.frame(name = x[1], 
             addres = result$response$docs$weergavenaam, 
             wkt = result$response$docs$centroide_rd, 
             score = result$response$docs$score, 
             stringsAsFactors = FALSE)
}

name <- c("Martinitoren", "Sint-Janskerk", "Onze-Lieve-Vrouwekathedraal")
address <- c("Martinikerkhof 3 Groningen", "Vrijthof 24 Maastricht", 
             "Groenplaats 21 Antwerpen")
poi <- data.frame(name, address)

poi <- apply(poi, 1, geocodeAddress) %>% 
       bind_rows() %>% 
       st_as_sf(wkt = 3, crs = 28992)
poi
```

Notice that the search result for the address of the Onze-Lieve-Vrouwe kathedraal (Cathedral of Our Lady) in Antwerp is wrong. According to the search result it is located in the Dutch town Doetinchem! Remember that Locatieserver will only return coordinates for addresses in The Netherlands. If it cannot find an exact match in its database, it will try to find an address that is similar. Every search result will have a score indicating the level of success. The score for the Onze-lieve-Vrouwe kathedraal is low. We should examine the scores of the search results and decide what to do with the low ones.

\newpage

# Appendix A: List of abbreviations used {-}

Abbreviation | Meaning
-------------|----------
API | application programming interface
CRAN | Comprehensive R Archive Network
GIS | Geographic Information System
GML | Geography Markup Language
IOGP | International Association of Oil & Gas Producers
JSON | JavaScript Object Notation
OGC | Open Geospatial Consortium
REST | Representational State Transfer
SaaS | software as a service
WFS | Web Feature Service
XML | eXtensible Markup Language

\newpage

# Appendix B: Additional exercises {-}

## OGC Web Feature Service (WFS) {-}

For background information on the steps taken in the exercise below, please refer to chapter \ref{chapWFS}

### `request=GetFeature` - another example from the Netherlands {-}

In this additional exercise we will access a WFS service offering information about *bevolkingskernen* (human settlements) in the Netherlands in 2011. More information on this dataset can be found here: [Dataset: CBS Bevolkingskernen](https://www.pdok.nl/introductie?articleid=1951745).

The URL with `GetCapabilities` request is:  
[https://geodata.nationaalgeoregister.nl/bevolkingskernen2011/wfs?request=GetCapabilities](https://geodata.nationaalgeoregister.nl/bevolkingskernen2011/wfs?request=GetCapabilities)

In the `<FeatureTypeList>` section of this XML response we can see that this service only offers one `<FeatureType>`, i.e. one dataset. This dataset has the `<Name>` **bevolkingskernen2011:cbsbevolkingskernen2011**. That's the value we are giving to the `typename` parameter in our `GetFeature` request.


The URL with `DescribeFeatureType` request is:  
[https://geodata.nationaalgeoregister.nl/bevolkingskernen2011/wfs?service=WFS&version=2.0.0&request=DescribeFeatureType&typename=bevolkingskernen2011:cbsbevolkingskernen2011](https://geodata.nationaalgeoregister.nl/bevolkingskernen2011/wfs?service=WFS&version=2.0.0&request=DescribeFeatureType&typename=bevolkingskernen2011:cbsbevolkingskernen2011)

This dataset does contain quite some attribute columns!

This is the script to build the full `GetFeature` request:

```{r}
# NL: Human Settlement Analysis
library(sf)
library(tmap)
library(httr)

url <- parse_url("https://geodata.nationaalgeoregister.nl/bevolkingskernen2011/wfs")
url$query <- list(service = "WFS",
                  version = "2.0.0",
                  request = "GetFeature",
                  typename = "bevolkingskernen2011:cbsbevolkingskernen2011",
                  outputFormat = "application/json")
request <- build_url(url)
```
The variable `request` will now contain this [link](https://geodata.nationaalgeoregister.nl/bevolkingskernen2011/wfs/?service=WFS&version=2.0.0&request=GetFeature&typename=bevolkingskernen2011%3Acbsbevolkingskernen2011&outputFormat=application%2Fjson). (When you click this link the server response will be shown in your browser in GeoJSON format.)

Now we want to feed this response directly into R, like this:

```{r}
NL_Human_Settlements2011 <- st_read(request)
```

```{r}
plot(st_geometry(NL_Human_Settlements2011), col = "orange", border = "red")
```

```{r echo = FALSE, results = 'hide'}
unlink("./Data", recursive = TRUE)
```
